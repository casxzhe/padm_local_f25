{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This problem set is due Wednesday, October 22, 2025 at 11:59 pm. Please plan ahead and submit your work on time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 06: MDPs\n",
    "\n",
    "In this problem set, you will implement value iteration and policy iteration in a non-deterministic MDP.\n",
    "\n",
    "\n",
    "0. [Credit for Contributors (required)](#contributors)\n",
    "\n",
    "1. [Value-based MDP Solutions (60 points)](#problem1)\n",
    "    1. [Value Convergence (5 points)](#convergence)\n",
    "    2. [Value Iteration (25 points)](#v_iteration)\n",
    "    3. [Policy Extraction (15 points)](#v_to_policy)\n",
    "    4. [Policy Observations (15 points)](#policy_observations)\n",
    "2. [Policy-based MDP Solutions (40 points)](#problem2)\n",
    "    1. [Policy Evaluation (15 points)](#policy_evaluation)\n",
    "    2. [Policy Improvement (15 points)](#policy_improvement)\n",
    "    3. [Policy Iteration (10 points)](#policy_iteration)\n",
    "3. [Time Spent on Pset (5 points)](#part4)\n",
    "    \n",
    "**100 points + 5 bonus** total for Problem Set 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to run the cell below to import the code needed for this assignment.\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from grid import MDPGrid, generate_mdp_plot, generate_grid_plot\n",
    "from mdp_utils import MDP, build_mdp\n",
    "\n",
    "# imports for autograder\n",
    "from principles_of_autonomy.grader import Grader\n",
    "from principles_of_autonomy.notebook_tests.pset_6 import TestPSet6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"contributors\"></a> 0. Credit for Contributors\n",
    "\n",
    "List the various students, lecture notes, or online resouces that helped you complete this problem set:\n",
    "\n",
    "Ex: I worked with Bob on the cat value iteration problem.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Write your answer in the cell below this one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> *(double click on this cell to delete this text and type your answer here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"problem1\"></a> 1. Value-based Approaches for Solving MDPs (60 points)\n",
    "\n",
    "In this problem, you will implement Value Iteration in a non-deterministic MDP grid world, together with a convergence function to check when the algorithm has converged. You will then implement Policy Extraction to compute the optimal policy from the optimal values calculated. Finally, you will plot the results and discuss your observations for different parameters of the MDP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are giving you a simple MDP class, which is defined in the `mdp_utils.py` file. You should open this file now to look at the class definition, especially at the properties of the MDP (S, A, T, R, $\\gamma$). The non-deterministic MDP model that we use is the same one described in chapter 17.1 of the AIMA book.\n",
    "\n",
    "Concretely, the robot is in a grid world and can perform four actions at any time: `up`, `down`, `right` and `left`. However, because it's not a deterministic world, the outcome of these actions is not perfect. After executing an action, the robot will move in its intended direction with probability `p`. However, with probability `1-p` it will move at right angles to the intended direction. Note that if the outcome of the action makes the robot bump into the walls of the world, the robot will bounce back and stay in the same state. For example, from the state `(1,1)`, the action `up` moves the agent to `(1,2)` with probability 0.8, but with probability 0.1, it moves right to `(2,1)`, and with probability 0.1, it moves left, bumps into the wall, and stays in `(1,1)`.\n",
    "\n",
    "The world is a `n x n` grid. There is a terminal goal cell. The robot receives reward `goal_reward` when it arrives in the goal cell. Similarly, there are obstacle cells. The robot receives (negative) reward `obstacle_reward` when it arrives in these cells. Both the goal and the obstacle cells are terminal: the robot can't leave these cells once it lands on them. Note: the grid world we saw in class required the agent to take an additional `Exit` action to collect its reward; we don't have that here - instead, the robot receives the reward immediately when arriving in the goal/obstacle cells.\n",
    "\n",
    "We are providing the function `build_mdp` that generates the MDP for this grid world:\n",
    "\n",
    "```python\n",
    "def build_mdp(n, p, obstacles, goal, gamma, goal_reward=100, obstacle_reward=-1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize what we're working with, let's see the MDP for a `3x3` grid with a goal state in `(2,2)` (green circle) and an obstacle in `(0,1)` (black circle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "goal = (2, 2)\n",
    "obstacles = [(0, 1)]\n",
    "\n",
    "# Build MDP with p=0.8 and gamma=0.8. Use default rewards.\n",
    "mdp = build_mdp(n, 0.8, obstacles, goal, 0.8)\n",
    "\n",
    "# Visualize the MDP:\n",
    "# 1. Create grid for plotting.\n",
    "g = MDPGrid(n, n)\n",
    "axes = g.draw()\n",
    "# 2. Draw goal and obstacle cells.\n",
    "g.draw_cell_circle(axes, goal, color='g')\n",
    "for ob in obstacles:\n",
    "    g.draw_cell_circle(axes, ob, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print information about the MDP that we just generated (MDP models get large pretty fast):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.print_mdp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"value_convergence\"></a> 1A. Check for Value Convergence (10 points)\n",
    "\n",
    "Before we dive into implementing Value Iteration, we will first get ourselves accustomed with some values, visualize them, then implement a convergence check that will come in handy when we implement the main Value Iteration algorithm. \n",
    "\n",
    "We will store values as a Python dictionary that maps each state in the MDP to a value. To see an example, here's what a random Value function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A random value function:\n",
    "def random_V(states):\n",
    "    # Initialize empty values dictionary.\n",
    "    V_random = dict()\n",
    "    # Generate random values between minV and maxV.\n",
    "    minV, maxV = -100, 100\n",
    "    for i, s in enumerate(states):\n",
    "        V_random[s] = np.random.uniform(minV, maxV)\n",
    "    return V_random\n",
    "\n",
    "# Print values generated with the random Value function.\n",
    "print(\"V_random is a valid Value function, although definitely not optimal (it's just random!)\")\n",
    "n = 3\n",
    "mdp = build_mdp(n, 0.8, [], (1, 1), 0.8)\n",
    "V_random = random_V(mdp.S)\n",
    "V_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize these values in the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid for plotting.\n",
    "g = MDPGrid(n, n)\n",
    "axes = g.draw()\n",
    "\n",
    "# Plot V in the grid.\n",
    "g.plot_V(axes, V_random, print_numbers = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration is, as the name implies, an iterative procedure. In class, we saw that we can run the algorithm for a fixed number of iterations; instead, we will run it until the values converge to within $\\epsilon$ from one iteration to another. Your task is to implement `value_convergence(V1, V2, epsilon)`, which takes 2 sets of values, $V_1$ and $V_2$, and checks whether for each state s the corresponding values $V_1(s)$ and $V_2(s)$ are within $\\epsilon$ from one another, i.e. $|V_1(s) - V_2(s)| <= \\epsilon, \\forall s\\in S$. If they are, the function should evaluate to True; otherwise return False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the function `value_convergence(V1, V2, epsilon)` below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_convergence(V1: dict, V2: dict, epsilon: float) -> bool:\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your value convergence code here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet6, \"test_1_convergence\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"v_iteration\"></a> 1B. Implement Value Iteration (25 points)\n",
    "\n",
    "In this part of the problem, you will implement **Value Iteration** for a given MDP. Your function should have the following signature:\n",
    "\n",
    "```python\n",
    "def value_iteration(mdp, epsilon=1e-3)\n",
    "```\n",
    "\n",
    "The function should take an MDP instance (like the one generated by the `build_mdp` function) and a value for `epsilon` that you'll use to determine when to stop value iteration with your new convergence function. We use by default $\\epsilon=10^{-3}$, but feel free to play around with the threshold and see how that changes the quality of your solution.\n",
    "\n",
    "The function should return a Python dictionary with the optimal value for each state in the MDP. Your function should also print how many iterations it took to reach convergence for the given `epsilon` value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the function `value_iteration(mdp, epsilon=1e-3)` below.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**Note**: Assume that if some some combination of `s`, `a` and `s_dest` is not found in the `reward` of the MDP (that is, you get an error when trying to access `mdp.R[s][a][s_dest]`, then the reward should be 0.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "value_iteration_sol",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Write your code for Value Iteration:\n",
    "def value_iteration(mdp: MDP, epsilon: float = 1e-3) -> dict:\n",
    "    \"\"\"\n",
    "    Implement value iteration.\n",
    "    Returns: V, a Python dictionary of values for each state.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out how your code performs on the initial grid world we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "check_value_iteration",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your code for value iteration with the example from before\n",
    "n = 3\n",
    "goal = (2, 2)\n",
    "obstacles = [(0, 1)]\n",
    "\n",
    "# Build MDP with p=0.8 and gamma=0.8. Use default rewards.\n",
    "mdp = build_mdp(n, 0.8, obstacles, goal, 0.8)\n",
    "\n",
    "# Perform value iteration on the MDP.\n",
    "V = value_iteration(mdp, epsilon=1e-3)\n",
    "\n",
    "# Visualize values:\n",
    "# 1. Create grid for plotting.\n",
    "g = MDPGrid(n, n)\n",
    "axes = g.draw()\n",
    "# 2. Plot values with colors and numbers.\n",
    "g.plot_V(axes, V, print_numbers=True)\n",
    "# 3. Draw goal and obstacle cells\n",
    "g.draw_cell_circle(axes, goal, color='g')\n",
    "for ob in obstacles:\n",
    "    g.draw_cell_circle(axes, ob, color='k')\n",
    "\n",
    "print(\"V:\\n\")\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your Value Iteration code here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet6, \"test_2_value_iteration\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"v_to_policy\"></a> 1C. Implement Policy Extraction (15 points)\n",
    "\n",
    "Values are useful because they enable us to compute policies so that the robot knows what to do at every particular state. In this part of the problem, you will implement **Policy Extraction**, a procedure which, given a values dictionary, returns the optimal policy induced by those values. \n",
    "\n",
    "We will store policies as a Python dictionary that maps each state in the MDP to an action. To see an example, here's what a random policy looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A random policy:\n",
    "def random_policy(states, actions):\n",
    "    # Initialize empty policy dictionary.\n",
    "    Pi_random = dict()\n",
    "    # Designate random actions at each state.\n",
    "    for i, s in enumerate(states):\n",
    "        Pi_random[s] = actions[int(np.random.randint(0, len(actions)))]    \n",
    "    return Pi_random\n",
    "\n",
    "# Generate random policy for grid with no obstacles and goal in the middle.\n",
    "n = 3\n",
    "mdp = build_mdp(n, 0.8, [], (1,1), 0.8)\n",
    "Pi_random = random_policy(mdp.S, mdp.A)\n",
    "\n",
    "# Visualize random policy.\n",
    "g = MDPGrid(n,n)\n",
    "axes = g.draw()\n",
    "g.plot_policy(axes, Pi_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function `extract_policy(mdp, V)` that extracts the optimal policy from V. Your function should have the following signature:\n",
    "\n",
    "```python\n",
    "def extract_policy(mdp, V)\n",
    "```\n",
    "\n",
    "The function takes an MDP instance and the values V out of which you'll extract the policy.\n",
    "\n",
    "The function should return a Python dictionary mapping each state to the best action to take (e.g. `up`,`down`,`right`,`left`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the function `extract_policy(mdp, V)` below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "extract_policy_code",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Write your code for Policy Extraction:\n",
    "def extract_policy(mdp: MDP, V: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extract the policy from the values dictionary.\n",
    "    Returns: Pi, a Python dictionary mapping states to actions.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test both your optimal values and your optimal policy in the example we have been using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "check_extract_policy",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test your code for Policy Extraction with the example from before.\n",
    "n = 3\n",
    "goal = (2,2)\n",
    "obstacles = [(0,1)]\n",
    "mdp = build_mdp(n, 0.8, obstacles, goal, 0.8)\n",
    "\n",
    "# Perform Value Iteration to get the optimal values first.\n",
    "V = value_iteration(mdp, epsilon=1e-3)\n",
    "# Extract the policy from the optimal values.\n",
    "Pi = extract_policy(mdp, V)\n",
    "\n",
    "# Visualize the result.\n",
    "# 1. Create grid for plotting\n",
    "g = MDPGrid(n, n)\n",
    "axes = g.draw()\n",
    "# 2. Plot the values and the policy.\n",
    "g.plot_V(axes, V, print_numbers=False)\n",
    "g.plot_policy(axes, Pi)\n",
    "# 3. Draw goal and obstacle cells\n",
    "g.draw_cell_circle(axes, goal, color='g')\n",
    "for ob in obstacles:\n",
    "    g.draw_cell_circle(axes, ob, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your Policy Extraction code here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet6, \"test_3_extract_policy\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"policy_observations\"></a> 1D. Observations (15 points)\n",
    "\n",
    "We will now run your code on a larger grid with more obstacles and ask you qualitative questions about your solution.\n",
    "Execute the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "complex_mdp_check",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "goal = (5,8)\n",
    "obstacles = [(1,3), (9,0), (8,8)] + \\\n",
    "            [(4, 2), (4, 3), (4, 6)] + \\\n",
    "            [(6, 2), (6, 3), (6, 5), (6, 6)]\n",
    "mdp = build_mdp(n, p=0.8, obstacles=obstacles, goal=goal, gamma=0.8, goal_reward=100, obstacle_reward=-500)\n",
    "V = value_iteration(mdp)\n",
    "Pi = extract_policy(mdp, V)\n",
    "generate_mdp_plot(mdp, V, Pi, obstacles, goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this policy look optimal? Does it look like the robot is doing something strange? Why? (hint: remember the transition model that we are using)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Write your answer in the cell below this one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "comment_mdp1",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "--> *(double click on this cell to delete this text and type your answer here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the previous example with a few variations of the values for `p` and `gamma`, such as:\n",
    "- `p = 1.0`, `gamma = 0.99`\n",
    "- `p = 1.0`, `gamma = 0.9`\n",
    "- `p = 1.0`, `gamma = 0.5`\n",
    "- `p = 0.8`, `gamma = 0.99`\n",
    "- `p = 0.8`, `gamma = 0.9`\n",
    "- `p = 0.8`, `gamma = 0.5`\n",
    "- `p = 0.4`, `gamma = 0.99`\n",
    "- `p = 0.4`, `gamma = 0.9`\n",
    "- `p = 0.4`, `gamma = 0.5`\n",
    "\n",
    "Think about these values affect the policy that we get. Match each combination of p and gamma to the most appropriate description of the policy behavior:\n",
    "\n",
    "- A) p = 1.0, gamma = 0.99         \n",
    "- B) p = 0.8, gamma = 0.5\t\n",
    "- C) p = 0.4, gamma = 0.9\t\n",
    "- D) p = 1.0, gamma = 0.5\t\n",
    "\n",
    "<br>\n",
    "\n",
    "- 1\\) Takes highly cautious paths to avoid uncertainty, but still values long-term rewards.       \n",
    "- 2\\) Takes moderately risky paths with short-term reward prioritization, balancing risk and reward.         \n",
    "- 3\\) Follows direct paths to maximize long-term rewards.       \n",
    "- 4\\) Follows direct paths, focusing on immediate rewards with little concern for the future. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Fill in the answer dictionary bellow with the corresponding numbers**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "comment_mdp2",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "answer = {\n",
    "    \"A\": 0,\n",
    "    \"B\": 0,\n",
    "    \"C\": 0,\n",
    "    \"D\": 0\n",
    "}\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your answer.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet6, \"test_4_observations\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"problem2\"></a> 2. Policy-based Approaches for Solving MDPs (40 points)\n",
    "\n",
    "You will now implement Policy Iteration in the same grid world environment, which involves implementing Policy Evaluation and Policy Improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"policy_evaluation\"></a> 2A. Implement Policy Evaluation (15 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the problem, you will implement **Policy Evaluation** for a given MDP. In class, we discussed two ideas for implementing this: one based on iterative updates like Value Iteration and one that uses a linear system solver. For this problem, you are NOT required to solve Policy Evaluation with a linear system solver (although you certainly can if you want to). Our cannonical solution assumes the iterative approach to Policy Evaluation.\n",
    "\n",
    "\n",
    "Your function should have the following signature:\n",
    "\n",
    "```python\n",
    "def policy_evaluation(mdp, policy, epsilon=1e-3)\n",
    "```\n",
    "\n",
    "The function takes an MDP instance, a policy to evaluate, and a value for `epsilon` to check for convergence between iterations (like in Value Iteration).\n",
    "\n",
    "The function should return a Python dictionary with the corresponding value for each state in the MDP. Your function should also print how many iterations it took to reach convergence for the given `epsilon` value. Note that these values are NOT optimal; they're just the values corresponding to whatever policy you're evaluating (which may be a random policy, which likely results in bad values). This function should look really similar to your `value_iteration` function from before (in fact, a good place to start is to just copy and paste your previous code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the function `policy_evaluation(mdp, policy, epsilon=1e-3)` below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(mdp: MDP, policy: dict, epsilon: float = 1e-3) -> dict:\n",
    "    \"\"\"\n",
    "    Perform policy evaluation to compute values for a given policy.\n",
    "    Returns: V, a Python dictionary of values for each state.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out how your code performs on the initial grid world we defined, with a random policy to evaluate. Run it a few times to convince yourself of the goodness (or badness) of the values corresponding to the random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code for Policy Evaluation with the example from before.\n",
    "n = 3\n",
    "goal = (2, 2)\n",
    "obstacles = [(0, 1)]\n",
    "\n",
    "# Build MDP with p=0.8 and gamma=0.8. Use default rewards.\n",
    "mdp = build_mdp(n, 0.8, obstacles, goal, 0.8)\n",
    "\n",
    "# Initialize random policy.\n",
    "Pi_random = random_policy(mdp.S, mdp.A)\n",
    "\n",
    "# Perform Policy Evaluation on the MDP.\n",
    "V = policy_evaluation(mdp, Pi_random, epsilon=1e-3)\n",
    "\n",
    "# Visualize values:\n",
    "generate_mdp_plot(mdp, V, Pi_random, obstacles, goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your Policy Evaluation code here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet6, \"test_5_policy_evaluation\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"policy_improvement\"></a> 2B. Implement Policy Improvement (15 points)\n",
    "\n",
    "In this part, you will implement **Policy Improvement** for a given MDP and a given policy, using the values that we get from evaluating that policy.\n",
    "\n",
    "Your function should have the following signature:\n",
    "\n",
    "```python\n",
    "def policy_improvement(mdp, policy, V)\n",
    "```\n",
    "\n",
    "The function takes an MDP instance, a policy to improve, and the resulting converged (but not optimal!) values.\n",
    "\n",
    "The function should return a Python dictionary mapping each state to the best action to take. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the function `policy_improvement(mdp, policy, V)` below.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**Note**: Do not modify the policy in place! Rather, return a new policy object.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(mdp: MDP, policy: dict, V: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Improve the policy based on the value function computed.\n",
    "    Returns: Pi, a Python dictionary mapping states to actions.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how your code performs on the initial grid world, starting from a random policy. Run it a few times to convince yourself that there's an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code for both Policy Evaluation and Policy Improvement with the example from before.\n",
    "n = 3\n",
    "goal = (2, 2)\n",
    "obstacles = [(0, 1)]\n",
    "\n",
    "# Build MDP with p=0.8 and gamma=0.8. Use default rewards.\n",
    "mdp = build_mdp(n, 0.8, obstacles, goal, 0.8)\n",
    "\n",
    "# Initialize random policy.\n",
    "Pi_random = random_policy(mdp.S, mdp.A)\n",
    "\n",
    "# Perform Policy Evaluation on the MDP.\n",
    "V = policy_evaluation(mdp, Pi_random, epsilon=1e-3)\n",
    "\n",
    "# Visualize values:\n",
    "generate_mdp_plot(mdp, V, Pi_random, obstacles, goal)\n",
    "\n",
    "# Perform Policy Improvement on the MDP with these evaluated values:\n",
    "Pi_improved = policy_improvement(mdp, Pi_random, V)\n",
    "\n",
    "# Visualize new policy:\n",
    "generate_mdp_plot(mdp, V, Pi_improved, obstacles, goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your Policy Improvement code here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet6, \"test_6_policy_improvement\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"policy_iteration\"></a> 2C. Implement Policy Iteration (10 points)\n",
    "\n",
    "We finally have what we need to implement **Policy Iteration**! Given an MDP, Policy Iteration iterates (as the name suggests) between Policy Evaluation and Policy Improvement. The algorithm stops when the improved policy matches the previous policy (there is no change from Policy Improvement).\n",
    "\n",
    "Your function should have the following signature:\n",
    "\n",
    "```python\n",
    "def policy_iteration(mdp, epsilon=1e-3)\n",
    "```\n",
    "\n",
    "The function takes an MDP instance and the `epsilon` needed for Policy Evaluation's convergence check.\n",
    "\n",
    "The function should return both the optimal policy and the optimal values: a Python dictionary mapping each state to the best action to take, and another Python dictionary mapping each state to the optimal values. \n",
    "\n",
    "We will initialize the starting policy for you with a random policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Implement the function `policy_iteration(mdp, epsilon=1e-3)` below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp: MDP, epsilon: float = 1e-3) -> tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Implement the full policy iteration algorithm.\n",
    "\n",
    "    Return a tuple of the optimal policy and the optimal values (policy, values)\n",
    "    \"\"\"\n",
    "    policy = random_policy(mdp.S, mdp.A)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how your code performs on the initial grid world, starting from a random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code for Policy Iteration with the example from before\n",
    "# Don't modify this cell (it may be overwritten)\n",
    "n = 3\n",
    "goal = (2, 2)\n",
    "obstacles = [(0, 1)]\n",
    "\n",
    "# Build MDP with p=0.8 and gamma=0.8. Use default rewards.\n",
    "mdp = build_mdp(n, 0.8, obstacles, goal, 0.8)\n",
    "\n",
    "# Perform Policy Iteration on the MDP.\n",
    "policy, V = policy_iteration(mdp)\n",
    "\n",
    "# Visualize values:\n",
    "generate_mdp_plot(mdp, V, policy, obstacles, goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also convince ourselves that the algorithm works on the large grid world. Execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "goal = (5,8)\n",
    "obstacles = [(1,3), (9,0), (8,8)] + \\\n",
    "            [(4, 2), (4, 3), (4, 6)] + \\\n",
    "            [(6, 2), (6, 3), (6, 5), (6, 6)]\n",
    "mdp = build_mdp(n, p=0.8, obstacles=obstacles, goal=goal, gamma=0.8, goal_reward=100, obstacle_reward=-500)\n",
    "policy, V = policy_iteration(mdp)\n",
    "generate_mdp_plot(mdp, V, policy, obstacles, goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test your Policy Evaluation code here.\"\"\"\n",
    "Grader.run_single_test_inline(TestPSet6, \"test_7_policy_iteration\", locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"part4\"></a> Time Spent on Pset (5 points)\n",
    "\n",
    "Please use [this form](https://forms.gle/JsdXpeLKTWjAUsht7) to tell us how long you spent on this pset. After you submit the form, the form will give you a confirmation word. Please enter that confirmation word below to get an extra 5 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_confirmation_word = \"ENTER_THE_CONFIRMATION_WORD_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tests\n",
    "Grader.grade_output([TestPSet6], [locals()], \"results.json\")\n",
    "Grader.print_test_results(\"results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you save the notebook before running this cell so that the most updated version is zipped!\n",
    "Grader.prepare_submission(\"ProblemSet06_MDPs_release\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
